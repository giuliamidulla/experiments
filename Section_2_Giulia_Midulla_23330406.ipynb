{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Giulia Midulla - 23330406"
      ],
      "metadata": {
        "id": "tJbXeuO3FbfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparatory steps"
      ],
      "metadata": {
        "id": "l2Y6WNR9J_jG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHtu47yEh3n",
        "outputId": "30e4e914-bf0e-4bae-96c5-2e193fc63f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary packages installed....\n"
          ]
        }
      ],
      "source": [
        "# The %pip commands below are for use if you have to install the packages\n",
        "# you can comment them out once you have installed them.\n",
        "# %pip install nltk\n",
        "# %pip install gensim\n",
        "import glob # string manipulation for constructing directory paths\n",
        "import nltk # bring in the Natural Language Tool Kit\n",
        "import gensim # bring in Gensim\n",
        "import os # handle Operating System file tasks\n",
        "import os.path # Used to determine if a file or directory exists\n",
        "import numpy as np # convenient mathematical handling\n",
        "from random import shuffle # facility to generate random selections\n",
        "from nltk.tokenize import TreebankWordTokenizer # String tokenizer\n",
        "from gensim import models\n",
        "from gensim.models.keyedvectors import KeyedVectors # word vector mapping\n",
        "# Keras network construction and handling libraries\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, SimpleRNN, Conv1D, GlobalMaxPooling1D\n",
        "print(\"Necessary packages installed....\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google drive mounted....\")\n",
        "\n",
        "working_directory = '/content/drive/MyDrive/Colab Notebooks/Sem 2/Section 2'\n",
        "\n",
        "if not os.path.exists(working_directory) :\n",
        "  os.mkdir(working_directory)\n",
        "  print(\"Created working directory....\")\n",
        "\n",
        "os.chdir(working_directory)\n",
        "print(\"Set current directory to working directory.... :\", working_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEh-eC35Ga4E",
        "outputId": "d4851fd5-8ae0-4553-fd99-3d8d68adf046"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google drive mounted....\n",
            "Set current directory to working directory.... : /content/drive/MyDrive/Colab Notebooks/Sem 2/Section 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"You will need to put the two downloaded compressed files into your working directory\", working_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBRztrgnHM-z",
        "outputId": "9d19a52d-b1d9-42de-c63f-5dc064ca76b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will need to put the two downloaded compressed files into your working directory /content/drive/MyDrive/Colab Notebooks/Sem 2/Section 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Stanford Sentiment Analysis files if you haven't already\n",
        "os.chdir(working_directory)\n",
        "print(\"Set current directory to working directory.... :\", working_directory)\n",
        "\n",
        "if os.path.exists('aclImdb_v1.tar.gz') :\n",
        "  !gunzip 'aclImdb_v1.tar.gz'\n",
        "\n",
        "if os.path.exists('aclImdb_v1.tar') :\n",
        "  !tar -xvf 'aclImdb_v1.tar'\n",
        "\n",
        "# Extract the Google News trained word vectors if you haven't already\n",
        "if os.path.exists('GoogleNews-vectors-negative300.bin.gz') :\n",
        "  !gunzip 'GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "#\n",
        "# If you need to save space on your Google Drive, then you can uncomment the\n",
        "# code below and delete the now unneeded compressed files\n",
        "#\n",
        "if os.path.exists(os.path.join(working_directory, 'aclImdb_v1.tar')) :\n",
        "   print(\"Removing Stanford directory tarball to save space....\")\n",
        "   os.remove(os.path.join(working_directory, 'aclImdb_v1.tar'))\n",
        "   print(\"Removed\")\n",
        "\n",
        "# Check directory contents\n",
        "print(\"The files in my working directory are\", os.listdir(working_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x_Hg_SAHujF",
        "outputId": "cf9758c7-0f82-4c5b-cf48-403795aa93c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set current directory to working directory.... : /content/drive/MyDrive/Colab Notebooks/Sem 2/Section 2\n",
            "The files in my working directory are ['GoogleNews-vectors-negative300.bin', 'aclImdb', 'prova.gdoc', 'prova cartella', 'cnn1_model.json', 'cnn1_weights.h5', 'model.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General Assignment Tasks"
      ],
      "metadata": {
        "id": "aS4vTvBJOjfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    This is fairly generic code for sentiment analysis cleaning but\n",
        "    it comes down to splitting the data into positive and negative\n",
        "    sentiments and labelling it accordingly with target values for\n",
        "    sentiment, 1 for positive and 0 for negative\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label,f.read()))\n",
        "\n",
        "    shuffle(dataset)\n",
        "\n",
        "    return(dataset)"
      ],
      "metadata": {
        "id": "OrOgvxy7OmV2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir(os.path.join(working_directory,\"aclImdb\"))\n",
        "#print(\"Acting on data in\",os.path.join(working_directory,'aclImdb') )\n",
        "dataset = pre_process_data(os.path.join(working_directory,\"aclImdb\",'train'))\n",
        "# Have a quick look at a sample\n",
        "dataset[42]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHLSS-SjOvNi",
        "outputId": "18ac017d-6b95-40a1-a773-dc46f0302e42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,\n",
              " \"I had hoped this movie was going to be mildly entertaining, like other sorts of its genre. However, it was lame and I didn't find myself laughing very much. Watch it on HBO, maybe, or if you've got a free rental to waste and you need a movie to pass the time. But I don't recommend paying to see it.<br /><br />The plot is simple and straightforward, and it could have been funny, maybe, if the script was better. Jason Lee can be hilarious, and he gets a few laughs here and there, but the movie falls flat. Just don't go see this one. The directing is lackluster, but for what it is, directing isn't that important. I guess its main drawback is that it is just not very funny. See something else, don't waste your time here.\")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit = 20000, binary=True)"
      ],
      "metadata": {
        "id": "Fl1TlPtqRjie"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(w[token])\n",
        "\n",
        "            except KeyError:\n",
        "                pass # No matching token in the downloaded Google word2vec vocab\n",
        "\n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data"
      ],
      "metadata": {
        "id": "5S_Dg_DDT0mu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\" Extract the target sentiments from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ],
      "metadata": {
        "id": "6z-Rd6eIT2uS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad or truncate each sequence of input tokens in each review so that we have\n",
        "# a fixed input length (**maxlen**) of tokens for network input\n",
        "\n",
        "def pad_trunc(data, maximumlen):\n",
        "    \"\"\"\n",
        "    Pad or truncate each review to the size set by the hyperparameter maxlen\n",
        "    because we need all the inputs of the CNN to be of fixed size.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maximumlen: # if the input is too large, truncate it\n",
        "            temp = sample[:maximumlen]\n",
        "        elif len(sample) < maximumlen: # if the input is too small, pad it\n",
        "            temp = sample\n",
        "            # Append the appropriate number zero vectors to the list\n",
        "            additional_elems = maximumlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "KB2FGEpeT5FB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "print(\"Number of vectorized training data -\", len(vectorized_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BsrAys0T7Ll",
        "outputId": "87fb2142-5172-443d-faca-06ace2a64745"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectorized training data - 4565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When we no longer need the dataset, we delete it\n",
        "# This is both good housekeeping and it stops\n",
        "# Colab from complaining about RAM or disk overrun\n",
        "del(dataset)"
      ],
      "metadata": {
        "id": "Zhtg6HMfUEId"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_point = int(len(vectorized_data)*.8)\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]\n",
        "\n",
        "print(\"Number of training vectors - \", len(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZvYbFk7UGtD",
        "outputId": "fc03983a-2243-40cf-dfdf-7049ac7dac0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training vectors -  3652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(vectorized_data) # Clear some more storage!\n",
        "print(\"Cleared out unneeded data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBz09rSmUITM",
        "outputId": "0570979e-2574-4f0f-d41f-afa5d9c53534"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared out unneeded data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "batch_size = 32\n",
        "embedding_dims = 300 # This is fixed as Google News used 300 dimensional vectors\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "XFamRPkfUKlv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad or truncate the reviews and convert the inputs to an optimized\n",
        "# Numpy format\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)   # Does the same with the test cases\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "3AdKFrGvUMer"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let people know what you are doing\n",
        "print('Build CNN model for our MN5002 Advanced NLP course....')\n",
        "model_cnn1 = Sequential()\n",
        "model_cnn1.add(Conv1D(\n",
        "    filters,\n",
        "    kernel_size,\n",
        "    padding='valid',\n",
        "    activation='relu',\n",
        "    strides=1,\n",
        "    input_shape=(maxlen, embedding_dims))\n",
        ")\n",
        "model_cnn1.add(GlobalMaxPooling1D())\n",
        "model_cnn1.add(Dense(hidden_dims)) # This is provides a fully connected hidden layer\n",
        "model_cnn1.add(Dropout(0.2)) # A dropout of 0.2 is added here\n",
        "model_cnn1.add(Activation('relu')) # The Rectified Linear Unit activation function is used here\n",
        "model_cnn1.add(Dense(1)) # Add 1 dense layer\n",
        "model_cnn1.add(Activation('sigmoid')) # Apply a sigmoid to the output of that layer to take the output to between 0 and 1\n",
        "print('Model built')\n",
        "model_cnn1.summary()\n",
        "model_cnn1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Model compiled')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVasYSGsUPJb",
        "outputId": "a46f0ecb-aeee-4cd3-8215-2641eedd28ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build CNN model for our MN5002 Advanced NLP course....\n",
            "Model built\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 98, 250)           225250    \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 250)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               62750     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250)               0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 250)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 288251 (1.10 MB)\n",
            "Trainable params: 288251 (1.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fu88chjUbn0",
        "outputId": "14ef9092-f700-4720-857f-c403a987e12c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "115/115 [==============================] - 6s 11ms/step - loss: 0.0207 - accuracy: 0.9904 - val_loss: 1.7721e-06 - val_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "115/115 [==============================] - 1s 6ms/step - loss: 9.8392e-07 - accuracy: 1.0000 - val_loss: 2.0693e-07 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a36c4ba7190>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(working_directory)\n",
        "model_cnn1_structure = model_cnn1.to_json()\n",
        "with open(\"cnn1_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_cnn1_structure)\n",
        "model_cnn1.save_weights(\"cnn1_weights.h5\")"
      ],
      "metadata": {
        "id": "nsqVd_bIUdjZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the module to read in a JSON format model\n",
        "# Remove the lines below with set of 3 inverted commas to enable code\n",
        "\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# Now instantiate a model\n",
        "\n",
        "os.chdir(working_directory)\n",
        "\n",
        "with open(\"cnn1_model.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model_cnn1 = model_from_json(json_string)\n",
        "\n",
        "# Once the model structure exists, set its characteristic weights\n",
        "\n",
        "model_cnn1.load_weights('cnn1_weights.h5')\n",
        "model_cnn1.summary()"
      ],
      "metadata": {
        "id": "Nv3uTyCtUgOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cc8633-5476-4306-bdea-0567cc0bf88b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 98, 250)           225250    \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 250)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               62750     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250)               0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 250)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 288251 (1.10 MB)\n",
            "Trainable params: 288251 (1.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Task 1"
      ],
      "metadata": {
        "id": "bESHQ0UsdMIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = 'this movie is terrible'\n",
        "print(test1, '- straightforward negative sentiment, used to check if the model works correctly\\n')\n",
        "test2 = 'this movie is wonderful, the best thing I\\'ve ever seen I believe it to be an undisputed masterpiece'\n",
        "print(test2, '- straightforward positive sentiment, used to check if the model works correctly\\n')\n",
        "test3 = 'this isn\\'t the best movie of all time'\n",
        "print(test3, '- negation of a positive sentiment, used to check the model\\'s reaction\\n')\n",
        "test4 = 'this movie is as good as The Room'\n",
        "print(test4, '- \\\"The Room\\\" is considered to be one of the worst movies ever made. This test is a trick\\n')\n",
        "test5 = 'how this movie is considered good will never fail to surprise me'\n",
        "print(test5, '- different type of trick: the reviewer is admitting that the movie is generally considered good, but they have an opposing opinion\\n')\n",
        "test6 = 'watching this movie will make you feel like holding a puppy or eating chocolate'\n",
        "print(test6, '- not explicit positive review\\n')\n",
        "test7 = 'Parasite is best described as a melancholy ghost story, albeit one disguised beneath umpteen layers of superbly designed (and impeccably photographed) generic mutations. Thrillingly played by a flawless ensemble cast who hit every note and harmonic resonance of Bong and co-writer Han Jin-won’s multitonal script, it’s a tragicomic masterclass that will get under your skin and eat away at your cinematic soul.'\n",
        "print(test7, '- long, glowing review (source: The Guardian)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YIl0AA7eKKX",
        "outputId": "eb6bfb11-3515-4dcc-bd22-8887af1077f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this movie is terrible - straightforward negative sentiment, used to check if the model works correctly\n",
            "\n",
            "this movie is wonderful, the best thing I've ever seen I believe it to be an undisputed masterpiece - straightforward positive sentiment, used to check if the model works correctly\n",
            "\n",
            "this isn't the best movie of all time - negation of a positive sentiment, used to check the model's reaction\n",
            "\n",
            "this movie is as good as The Room - \"The Room\" is considered to be one of the worst movies ever made. This test is a trick\n",
            "\n",
            "how this movie is considered good will never fail to surprise me - different type of trick: the reviewer is admitting that the movie is generally considered good, but they have an opposing opinion\n",
            "\n",
            "watching this movie will make you feel like holding a puppy or eating chocolate - not explicit positive review\n",
            "\n",
            "Parasite is best described as a melancholy ghost story, albeit one disguised beneath umpteen layers of superbly designed (and impeccably photographed) generic mutations. Thrillingly played by a flawless ensemble cast who hit every note and harmonic resonance of Bong and co-writer Han Jin-won’s multitonal script, it’s a tragicomic masterclass that will get under your skin and eat away at your cinematic soul. - long, glowing review (source: The Guardian)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Task 2"
      ],
      "metadata": {
        "id": "e4r5T_XDlBoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test 1"
      ],
      "metadata": {
        "id": "tMGS3QphogcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first test I'll use the preset hyperparameters:\n",
        "```\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "embedding_dims = 300 # This is fixed as Google News used 300 dimensional vectors\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2\n",
        "```\n"
      ],
      "metadata": {
        "id": "yTm-KBKDojj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a sample\n",
        "sample_2 =\"This is one of the worst movies I have ever seen. I am surprised anyone has recommended it, it is as bad as they get.\"\n",
        "\n",
        "vec_list = tokenize_and_vectorize([(1, sample_2)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TFkQPeGlDZp",
        "outputId": "ef83a9b1-a6ab-495a-e6a7-4d09d43a8388"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 155ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.1847078e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK"
      ],
      "metadata": {
        "id": "4Z57_012dD3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test1)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdu1KtHymLBW",
        "outputId": "cafcba18-11e3-4fa6-e766-563c82627203"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.237688e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK"
      ],
      "metadata": {
        "id": "6afWXGKRdF1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test2)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7OBVg38mO_o",
        "outputId": "fe69e6f3-fe01-456b-eb03-43a70b691dd2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.1365333e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - NO"
      ],
      "metadata": {
        "id": "uLeisH6cdKRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test3)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5guaAH3OnctI",
        "outputId": "2250e529-6913-48e0-eb8d-f8ac22225ecf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.1304855e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK"
      ],
      "metadata": {
        "id": "plDw_tcndR5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test4)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD3xE7I_ngH1",
        "outputId": "1e39e478-f927-42cc-b9c4-df3c22000300"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.6613036e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK"
      ],
      "metadata": {
        "id": "1N0s2d3nddtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test5)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nBBeokQnh1o",
        "outputId": "33ff3dd8-186b-4af9-b6f4-dafdba060517"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.9896473e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK"
      ],
      "metadata": {
        "id": "vWitjcOOdmc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test6)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmVkTI7njrO",
        "outputId": "7566df34-b9bc-4165-923b-d93748e94c58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.6771414e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - NO"
      ],
      "metadata": {
        "id": "RnRiR0jQdo16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test7)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPb6s5se2go",
        "outputId": "ad7aa6a7-252a-4d47-9f7f-d4ce7714eebf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.467729e-07]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1 only returns negative values. It also doesn't seem like positive reviews are closer to 1 than negative reviews."
      ],
      "metadata": {
        "id": "n6GFuiOYmoAb"
      }
    }
  ]
}