{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u88IQgpZKMmr",
        "outputId": "6a7c513d-7cbe-456d-835c-8473f13312e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade gensim\n",
        "import gensim\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utnUno__Q0LJ",
        "outputId": "8c73fc82-1804-46dd-81f7-25fcc385b72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z49rk6stS0iK"
      },
      "source": [
        "#Knowing that the capital of France is Paris, use vector reasoning to find the capital of Germany and the capital of Australia and explain the answers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the capital of Germany and Australia knowing that the capital of France is Paris, I used `most_similar`and `most_similar_cosul`. The positive terms will be \"Paris\" and the country whose capital I'm looking for, and the negative term will be \"France\". The aim is to remove the characteristic \"France\" from Paris, and be left with the characteristic \"capital\" which, in conjunction with the target country, should return the capital of said country."
      ],
      "metadata": {
        "id": "yKU44othW5bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7cydA85ThSJ",
        "outputId": "c503d7de-bba5-4376-9fa1-ccdd955383bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berlin: 0.7644\n"
          ]
        }
      ],
      "source": [
        "result = google_news_vectors.most_similar(positive=['Germany', 'Paris'], negative=['France'])\n",
        "most_similar_key, similarity = result[0]  # look at the first match\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuPDw1Hkmq5k",
        "outputId": "65b4e606-6ce1-485d-dc2e-c7572fa76655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berlin: 0.8890\n"
          ]
        }
      ],
      "source": [
        "result = google_news_vectors.most_similar_cosmul(positive=['Germany', 'Paris'], negative=['France'])\n",
        "most_similar_key, similarity = result[0]\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip4l_wM4Tq3F",
        "outputId": "da73cdd6-63c9-4f72-c035-6de8f561bab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sydney: 0.7721\n"
          ]
        }
      ],
      "source": [
        "result = google_news_vectors.most_similar(positive=['Australia', 'Paris'], negative=['France'])\n",
        "most_similar_key, similarity = result[0]  # look at the first match\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4B5W7vmmd04",
        "outputId": "c640d6d7-0297-4d60-d651-5c90fd91ce7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sydney: 0.9873\n"
          ]
        }
      ],
      "source": [
        "result = google_news_vectors.most_similar_cosmul(positive=['Australia', 'Paris'], negative=['France'])\n",
        "most_similar_key, similarity = result[0]\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the reasoning worked for Germany, it did not work as expected for Australia. I believe the issue is that Paris is not only the capital of France, but also its most iconic city. Therefore the model seems to have returned the most iconic city of Australia, instead of its capital. This can be fixed by changing Paris/France with a capital that's not the most famous city of the country it's in. For example, Ankara/Turkey:"
      ],
      "metadata": {
        "id": "WJHjrvouX8MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = google_news_vectors.most_similar(positive=['Australia', 'Ankara'], negative=['Turkey'])\n",
        "most_similar_key, similarity = result[0]  # look at the first match\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWzfbVuJmnFa",
        "outputId": "d6ded13c-13e9-4f69-cae3-59b9f42b2dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canberra: 0.7460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = google_news_vectors.most_similar_cosmul(positive=['Australia', 'Ankara'], negative=['Turkey'])\n",
        "most_similar_key, similarity = result[0]\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rUwp9SEmtd3",
        "outputId": "eb196610-d00b-4a88-ad61-ad0a3e4ec489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canberra: 0.9566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This modification worked. This confirms my suspicion about Paris being recognised as the most famous city of France, instead of its capital."
      ],
      "metadata": {
        "id": "wugN65tbYm7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#By considering the USA and by considering Russia, use vector reasoning to find the UK Prime Minister from the corpus and explain the answers you get."
      ],
      "metadata": {
        "id": "ms_7S96YCBmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this exercise, I plan on using `most_similar_cosmul` to add three variables: the name of the American president, the name of the Russian president, and \"UK\". I will then subtract the variables \"USA\" and \"Russia\", which should hopefully output the political leader of the UK, Sunak.\n"
      ],
      "metadata": {
        "id": "ZqrjBuIs5NZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.most_similar_cosmul(positive=['Biden', 'Putin', 'UK'], negative=['USA', 'Russia'], topn = 10)"
      ],
      "metadata": {
        "id": "0NKVy0PmCCtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a77230f-8c28-4d63-e9bd-c109997fc887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Miliband', 1.1504093408584595),\n",
              " ('Mr_Clegg', 1.1192729473114014),\n",
              " ('Mr_Blunkett', 1.116330862045288),\n",
              " ('Blunkett', 1.0937331914901733),\n",
              " ('Tony_Blair', 1.0924168825149536),\n",
              " ('Mr_Straw', 1.0922507047653198),\n",
              " ('Mr_Raynsford', 1.0901881456375122),\n",
              " ('Mr_Bercow', 1.08929443359375),\n",
              " ('Sir_Menzies', 1.0771187543869019),\n",
              " ('Ed_Miliband', 1.0750203132629395)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first attempt returns Miliband as the first output. Miliband was a prominent political figure between 2010 and 2015, but was never PM.\n",
        "\n",
        "The corpus might not be updated. Let's double check if it knows that Biden is the current president of the US:"
      ],
      "metadata": {
        "id": "ni6k89Vc545U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Biden\", topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKMM2yJZ4R8G",
        "outputId": "d645e00b-3e93-4c72-ef7a-46e6b9444aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Joe_Biden', 0.826562762260437),\n",
              " ('Joseph_Biden', 0.7567954063415527),\n",
              " ('Obama', 0.7485177516937256),\n",
              " ('McCain', 0.7168547511100769),\n",
              " ('Sen._Joe_Biden', 0.7074717879295349),\n",
              " ('Cheney', 0.6439098715782166),\n",
              " ('Bayh', 0.6293529272079468),\n",
              " ('Illinois_senator', 0.6248633861541748),\n",
              " ('Clinton', 0.6244214773178101),\n",
              " ('elect_Joe_Biden', 0.6210129857063293)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like, according to the corpus, Biden is still considered the vice-president. There are a few factors that lead me to believe this: first of all, no output contains the word \"president\". Second, there are two instances out of ten of the word \"senator\". Third, Obama is the third output right after the variations of Biden's full name. This shows that the corpus still strongly associates Biden and Obama, which corroborates my theory that its last update preceeds Biden's election.\n",
        "\n",
        "Let's try with the previous president, Trump:"
      ],
      "metadata": {
        "id": "vFFJREcl6WgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.most_similar_cosmul(positive=['Trump', 'Putin', 'UK'], negative=['USA', 'Russia'], topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY_iKK_h4O0O",
        "outputId": "37d398c0-2a5b-48ae-9035-e7f11775508a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mr_Blunkett', 1.088681697845459),\n",
              " ('Blunkett', 1.0651389360427856),\n",
              " ('Luminar', 1.0258002281188965),\n",
              " ('Mr_Sorial', 1.0237212181091309),\n",
              " ('Worrall_Thompson', 1.0231560468673706),\n",
              " ('Mr_Raynsford', 1.0231468677520752),\n",
              " ('Sir_Gus', 1.0218018293380737),\n",
              " ('shadow_chancellor', 1.0198085308074951),\n",
              " ('Giles_Thorley', 1.0119723081588745),\n",
              " ('Ritblat', 1.0113977193832397)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the code doesn't return the expected output. The first result is Mr. Blunkett, a British politician who has never been PM.\n",
        "\n",
        "Let's check if the corpus has any records of Trump being president:"
      ],
      "metadata": {
        "id": "OgVDjhVC7cQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Trump\", topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I8gjtSE4bqD",
        "outputId": "263d22af-8077-4205-a690-b87f131991be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Donald_Trump', 0.8103920221328735),\n",
              " ('impersonator_entertained', 0.5942257642745972),\n",
              " ('Ivanka_Trump', 0.5924582481384277),\n",
              " ('Ivanka', 0.560720682144165),\n",
              " ('mogul_Donald_Trump', 0.559245228767395),\n",
              " ('Trump_Tower', 0.548555314540863),\n",
              " ('Kepcher', 0.5468589067459106),\n",
              " ('billionaire_Donald_Trump', 0.5447270274162292),\n",
              " ('Trumpster', 0.5412819385528564),\n",
              " ('tycoon_Donald_Trump', 0.5383971929550171)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like Trump is still known by the corpus as a mogul and celebrity. Let's go back to Obama:"
      ],
      "metadata": {
        "id": "B-_J02Ju8DNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.most_similar_cosmul(positive=['Obama', 'Putin', 'UK'], negative=['USA', 'Russia'], topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKaocVhv4gIp",
        "outputId": "6328897a-3cc8-4c66-fddd-3b7dc695d867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Miliband', 1.1586889028549194),\n",
              " ('Mr_Clegg', 1.1437619924545288),\n",
              " ('Tony_Blair', 1.124590277671814),\n",
              " ('Ed_Miliband', 1.1243938207626343),\n",
              " ('Gordon_Brown', 1.1196506023406982),\n",
              " ('Mr_Blunkett', 1.104709267616272),\n",
              " ('Mr_Raynsford', 1.100437045097351),\n",
              " ('Blunkett', 1.0951956510543823),\n",
              " ('Mr_Letwin', 1.0878994464874268),\n",
              " ('shadow_chancellor', 1.0824898481369019)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is similar to the one we got using \"Biden\", but this time former PM Tony Blair is in a higher position in the list. Let's double check if Obama is the president according to the corpus:"
      ],
      "metadata": {
        "id": "b9WCV6P98YFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Obama\", topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyQaq4Jp42B8",
        "outputId": "b1dab748-374a-414f-d4a5-9ca4d3fa8422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Barack_Obama', 0.8036513328552246),\n",
              " ('President_Barack_Obama', 0.7878769040107727),\n",
              " ('McCain', 0.7555227875709534),\n",
              " ('Clinton', 0.7526832818984985),\n",
              " ('Illinois_senator', 0.7497453093528748),\n",
              " ('Biden', 0.7485178709030151),\n",
              " ('Bush', 0.7348896265029907),\n",
              " ('Barack', 0.7290467619895935),\n",
              " ('White_House', 0.7151209115982056),\n",
              " ('elect_Barack_Obama', 0.6941338181495667)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second result is \"President Barack Obama\", and the ninth is \"White House\". We can confindently deduct that Obama is the last American president recorded in the Google corpus. Now let's try a variation of the previous query, using Obama and Putin's full names:"
      ],
      "metadata": {
        "id": "0Nkdds7K8mWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.most_similar_cosmul(positive=['Barack_Obama', 'Vladimir_Putin', 'UK'], negative=['USA', 'Russia'], topn = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EigQqYCp4xJw",
        "outputId": "64952743-653a-4225-ec7b-e2331d50e43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tony_Blair', 1.199454426765442),\n",
              " ('Gordon_Brown', 1.1514054536819458),\n",
              " ('Ed_Miliband', 1.0818523168563843),\n",
              " ('Mark_Latham', 1.0468589067459106),\n",
              " ('GORDON_Brown', 1.0407851934432983),\n",
              " ('Alan_Milburn', 1.0367484092712402),\n",
              " ('Nicolas_Sarkozy', 1.0321784019470215),\n",
              " ('shadow_chancellor', 1.0303831100463867),\n",
              " ('Alistair_Darling', 1.027529239654541),\n",
              " ('Mr_Clegg', 1.0268748998641968)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is \"Tony Blair\", former British PM, who resigned a couple of years before Obama was elected. The second result is \"Gordon Brown\", his successor, whose role as PM overlapped with Obama's presidency.\n",
        "\n",
        "We can assume that the corpus was last updated between 2009 and 2017, the duration of Obama's mandate. By cross referencing with other politicians, artists and events, we could eventually pinpoint the exact date of the last update."
      ],
      "metadata": {
        "id": "nWYHxJyf9H0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What are the 5 most similar words to the word BMW? Explain your answer."
      ],
      "metadata": {
        "id": "Sv6Ciic6Atk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"BMW\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fng59-qMAxuz",
        "outputId": "04594dde-2f40-4f2d-bcc7-6c99efd5a450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Audi', 0.7932199835777283),\n",
              " ('Mercedes_Benz', 0.7683466672897339),\n",
              " ('Porsche', 0.7272197604179382),\n",
              " ('Mercedes', 0.7078384160995483),\n",
              " ('Volkswagen', 0.6959410905838013)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most similar words to \"BMW\" are other car manufacturing companies. This is an expected result. All the results belong to the same domain as the input word."
      ],
      "metadata": {
        "id": "YmX7w_SHY4Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Tesla\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e2JOOkKB3FS",
        "outputId": "001068a7-ccc9-4d72-85c1-5b733fa31909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tesla_Motors', 0.720951497554779),\n",
              " ('Tesla_Roadster', 0.6531893014907837),\n",
              " ('afford_Nummi_Musk', 0.65052330493927),\n",
              " ('Telsa', 0.6308883428573608),\n",
              " ('electric_Tesla_Roadster', 0.6106851696968079)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words related to \"Tesla\" are all related to Tesla itself. There are mentions of a car model, the CEO, and a company he bought (NUMMI). It is interesting to note what seems to be a typo, \"Telsa\". I believe it would be useful to implement `.similar_by_word`in a spell-check algorithm.\n"
      ],
      "metadata": {
        "id": "-QB4hXgTZStg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which of the words battle and love are closest to the word fight? Explain your answer."
      ],
      "metadata": {
        "id": "7orfrwTaCVDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = google_news_vectors.similarity('fight', 'battle')\n",
        "print(similarity)\n",
        "\n",
        "similarity = google_news_vectors.similarity('fight', 'love')\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjJVm2UQCWn1",
        "outputId": "15c0fc2d-0d7a-4aef-a469-8c05aabf0164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7021284\n",
            "0.13506128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Fight\" is a lot more similar to \"battle\" than to \"love\". This is not surprising, since \"fight\" and \"battle\" both relate to violence, contrast, and hatred."
      ],
      "metadata": {
        "id": "ViBnljD-aTYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explore the corpus for gender bias in science. Start by considering that Albert Einstein is probably the most famous scientist. If you detect bias, elaborate on why it may be present, with specific examples, and what steps could be taken to address such bias."
      ],
      "metadata": {
        "id": "shUHKhVVCuYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check words most similar to AE\n",
        "* Check words most similar to other famous scientists\n",
        "* Compare words similar to AE to words similar to Marie Curie and Rosalind Franklin\n",
        "* Detect odd one out: 2 male scientists, 1 female scientist, 1 male historical figure. Is it going to give more weight to gender or to profession?\n",
        "* Check similarity between male scientist / \"science\" and female scientist / \"science\", also between \"man\" and \"female\" with \"science\""
      ],
      "metadata": {
        "id": "jA0UFqaUCyTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First test: check words most similar to 3 male and 3 female scientists"
      ],
      "metadata": {
        "id": "H9c7SmuyOrW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Albert_Einstein\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk7t2mJjN7FW",
        "outputId": "579e15cd-44ae-4c38-b486-19e91e1339d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Einstein', 0.7338969111442566),\n",
              " ('physicist_Albert_Einstein', 0.6331198811531067),\n",
              " ('Satyendra_Nath_Bose', 0.5870295763015747),\n",
              " ('Erwin_Schrödinger', 0.5869701504707336),\n",
              " ('Stephen_Hawking', 0.5820940136909485)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Stephen_Hawking\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkP2SMNbOGE0",
        "outputId": "4e529b08-c0ad-4e05-805a-77d7a41c8d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hawking', 0.6998317837715149),\n",
              " ('Professor_Stephen_Hawking', 0.6878412365913391),\n",
              " ('Renowned_physicist', 0.6720010638237),\n",
              " ('theoretical_physicist_Stephen_Hawking', 0.6458166241645813),\n",
              " ('physicist_Stephen_Hawking', 0.6423981189727783)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Isaac_Newton\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8xMx6xIOJuf",
        "outputId": "76d5822a-504b-46cf-cdfb-1e3ebea3c6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Issac_Newton', 0.6686496734619141),\n",
              " ('universal_gravitation', 0.5819745659828186),\n",
              " ('Albert_Einstein', 0.5639371275901794),\n",
              " ('Roger_Penrose', 0.5441792607307434),\n",
              " ('Principia_Mathematica', 0.5362484455108643)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Rosalind_Franklin\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GIrtAYjOWYM",
        "outputId": "cf7aae0a-1524-4610-da11-29f1bbe0f9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Rustem_Ismagilov', 0.48224276304244995),\n",
              " ('Quanterix', 0.4664601683616638),\n",
              " ('Dorothy_Hodgkin', 0.46534794569015503),\n",
              " ('Marvin_Caruthers', 0.45950573682785034),\n",
              " ('Seymour_Benzer', 0.45932093262672424)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Marie_Curie\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stmyvDkMOa2G",
        "outputId": "9e3e6082-88e6-4375-ba28-4f59a54646f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Leukaemia_Research', 0.5468690395355225),\n",
              " ('Children_Liver_Disease', 0.5446346998214722),\n",
              " ('Macmillan_Cancer_Care', 0.5297462344169617),\n",
              " ('Cystic_Fibrosis_Trust', 0.5266525149345398),\n",
              " ('Macmillan_Cancer', 0.5199000835418701)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors.similar_by_word(\"Dorothy_Hodgkin\", topn = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZjqdBZhOdnu",
        "outputId": "86e8b0b2-9c60-49eb-e070-ae4830e031ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Balzan_Prize', 0.6231822967529297),\n",
              " ('Harold_Kroto', 0.6160528659820557),\n",
              " ('Renato_Dulbecco', 0.6095486283302307),\n",
              " ('David_Baulcombe', 0.6060463786125183),\n",
              " ('bioorganic_chemistry', 0.5987771153450012)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first test doesn't return significant information. The results are very similar, and all relate to either other scientists or the scientist's accomplishment and field of study.\n",
        "\n",
        "Second test: detect odd one out. The prompt will contain a couple of male scientists, one female scientist, and one male historical figure. The aim is to see if the model gives more importance to gender or to profession.\n"
      ],
      "metadata": {
        "id": "rXRjBqGsO5eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_news_vectors.doesnt_match(\"Einstein Hawking Curie Kennedy\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz-iwzuCPd6x",
        "outputId": "c9946f4d-5fae-41cf-a185-8b1e14a65868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kennedy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The politician is the odd one out"
      ],
      "metadata": {
        "id": "d7yX30raS37w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_news_vectors.doesnt_match(\"Einstein Hawking Curie Freud\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6UuFiCoQDa1",
        "outputId": "e0895456-4391-48f9-80b9-f0faf3234fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite Freud being a psychologist and not a scientist, Curie is the odd one out"
      ],
      "metadata": {
        "id": "xGmNYHeVS8Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_news_vectors.doesnt_match(\"Newton Aristotle Hodgkin Napoleon\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qtbNW8LQHhB",
        "outputId": "99dd14f6-2731-44be-a3e6-9cc0c219c649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hodgkin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this test I used figures from very different eras. Hodgkin is the odd one out, despite the fact that Napoleon is the only figure that's not a scientist"
      ],
      "metadata": {
        "id": "6kDDSKi9TBrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_news_vectors.doesnt_match(\"Ada_Lovelace Alan_Turing Steve_Jobs Pablo_Picasso\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-RdU-aIQncj",
        "outputId": "1cf6422b-6bf8-4e7d-b060-18ce96530e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pablo_Picasso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test correctly outputs Pablo Picasso as the odd one out, seen as he is a painter and the rest are scientist"
      ],
      "metadata": {
        "id": "dxttubUPTRbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_news_vectors.doesnt_match(\"Hypatia Archimedes Aristotle Homer\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmRCQXvgSLfl",
        "outputId": "ea78c37a-b0b0-40b3-cc35-d2034d68f3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test correctly recognises the poet Homer as the odd one out"
      ],
      "metadata": {
        "id": "0ho8dZDtS2tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 3 out of 5 tests, the model doesn't seem to be biased against female scientists.\n",
        "\n",
        "Third test: compare male and female scientists with \"science\"."
      ],
      "metadata": {
        "id": "CqgzSay2TkqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "male_similarity = 0\n",
        "female_similarity = 0\n",
        "\n",
        "similarity = google_news_vectors.similarity('Einstein', 'science')\n",
        "print('Einstein/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('Hawking', 'science')\n",
        "print('Hawking/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('Aristotle', 'science')\n",
        "print('Aristotle/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('Curie', 'science')\n",
        "print('Curie/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('Hodgkin', 'science')\n",
        "print('Hodgkin/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('Lovelace', 'science')\n",
        "print('Lovelace/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "print('\\nAverage similarity to \"science\" of male scientists: ', male_similarity/3)\n",
        "print('Average similarity to \"science\" of female scientists: ', female_similarity/3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvyl-TzsT6u6",
        "outputId": "da81cf22-d90b-4a8f-824c-8534bf5e674f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Einstein/science: 0.3274904\n",
            "Hawking/science: 0.2655893\n",
            "Aristotle/science: 0.25090933\n",
            "Curie/science: 0.1749896\n",
            "Hodgkin/science: 0.029929686\n",
            "Lovelace/science: 0.09836112\n",
            "\n",
            "Average similarity to \"science\" of male scientists:  0.2813296715418498\n",
            "Average similarity to \"science\" of female scientists:  0.10109346732497215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_similarity = 0\n",
        "female_similarity = 0\n",
        "\n",
        "similarity = google_news_vectors.similarity('man', 'science')\n",
        "print('man/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('boy', 'science')\n",
        "print('boy/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('male', 'science')\n",
        "print('male/science:', similarity)\n",
        "\n",
        "male_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('woman', 'science')\n",
        "print('woman/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('girl', 'science')\n",
        "print('girl/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "similarity = google_news_vectors.similarity('female', 'science')\n",
        "print('female/science:', similarity)\n",
        "\n",
        "female_similarity += similarity\n",
        "\n",
        "print('\\nAverage similarity to \"science\" of masculine words: ', male_similarity/3)\n",
        "print('Average similarity to \"science\" of feminine words: ', female_similarity/3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRzgzYP8VR23",
        "outputId": "58a43262-2ef5-4f9c-da48-2fe194de141c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man/science: 0.025733905\n",
            "boy/science: 0.0484591\n",
            "male/science: 0.03838287\n",
            "woman/science: 0.04712866\n",
            "girl/science: 0.079624794\n",
            "female/science: 0.07775529\n",
            "\n",
            "Average similarity to \"science\" of masculine words:  0.03752529186507066\n",
            "Average similarity to \"science\" of feminine words:  0.06816958015163739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Male scientists seem to have a higher similarity with the word \"science\" than female scientists. On the other hand, words describing men have a lower similarity to science than words describing women.\n",
        "\n",
        "Of course, many more tests would be required to draw accurate conclusions about the possible gender bias of this corpus. From the tests I ran, it seems like the model isn't particularly biased. The bias against female scientists does exist but it's not particularly strong."
      ],
      "metadata": {
        "id": "euabDXaFVwqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How is a word vector represented as a Python data structure? Explain with an example from the corpus.\n"
      ],
      "metadata": {
        "id": "YixxnWeGDqCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word vector is represented as an array of dimensions, all of which are 0 except for one which is 1. The Google News vectors are made up of 300 dimensions:"
      ],
      "metadata": {
        "id": "tXikEVqaZU7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = google_news_vectors['computer']\n",
        "vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkKkzRJbDtrk",
        "outputId": "558540f5-5338-46a4-a578-a346fdfdfa57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLLLB6j_URPI"
      },
      "source": [
        "#Which of these words is the odd one out? California Texas Alaska India"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_C3MGBTUSX5",
        "outputId": "4daf8e8a-aad0-40c0-a11b-88d6678fa3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India\n"
          ]
        }
      ],
      "source": [
        "print(google_news_vectors.doesnt_match(\"California Texas Alaska India\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is expected. India is the odd one out, because it is the only country in a list of US states."
      ],
      "metadata": {
        "id": "Qvz30Lttao1q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
