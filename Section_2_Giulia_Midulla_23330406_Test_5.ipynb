{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Giulia Midulla - 23330406"
      ],
      "metadata": {
        "id": "tJbXeuO3FbfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparatory steps"
      ],
      "metadata": {
        "id": "l2Y6WNR9J_jG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHtu47yEh3n",
        "outputId": "a7647763-758a-4b92-e1f3-36f7267d86bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary packages installed....\n"
          ]
        }
      ],
      "source": [
        "# The %pip commands below are for use if you have to install the packages\n",
        "# you can comment them out once you have installed them.\n",
        "# %pip install nltk\n",
        "# %pip install gensim\n",
        "import glob # string manipulation for constructing directory paths\n",
        "import nltk # bring in the Natural Language Tool Kit\n",
        "import gensim # bring in Gensim\n",
        "import os # handle Operating System file tasks\n",
        "import os.path # Used to determine if a file or directory exists\n",
        "import numpy as np # convenient mathematical handling\n",
        "from random import shuffle # facility to generate random selections\n",
        "from nltk.tokenize import TreebankWordTokenizer # String tokenizer\n",
        "from gensim import models\n",
        "from gensim.models.keyedvectors import KeyedVectors # word vector mapping\n",
        "# Keras network construction and handling libraries\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, SimpleRNN, Conv1D, GlobalMaxPooling1D\n",
        "print(\"Necessary packages installed....\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google drive mounted....\")\n",
        "\n",
        "working_directory = '/content/drive/MyDrive/Colab_Notebooks/Sem_2/Section_2'\n",
        "\n",
        "if not os.path.exists(working_directory) :\n",
        "  os.mkdir(working_directory)\n",
        "  print(\"Created working directory....\")\n",
        "\n",
        "os.chdir(working_directory)\n",
        "print(\"Set current directory to working directory.... :\", working_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEh-eC35Ga4E",
        "outputId": "61633b6f-4573-45a0-a3f0-4442ec179607"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google drive mounted....\n",
            "Set current directory to working directory.... : /content/drive/MyDrive/Colab_Notebooks/Sem_2/Section_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Stanford Sentiment Analysis files if you haven't already\n",
        "os.chdir(working_directory)\n",
        "print(\"Set current directory to working directory.... :\", working_directory)\n",
        "\n",
        "if os.path.exists('aclImdb_v1.tar.gz') :\n",
        "  !gunzip 'aclImdb_v1.tar.gz'\n",
        "\n",
        "if os.path.exists('aclImdb_v1.tar') :\n",
        "  !tar -xvf 'aclImdb_v1.tar'\n",
        "\n",
        "# Extract the Google News trained word vectors if you haven't already\n",
        "if os.path.exists('GoogleNews-vectors-negative300.bin.gz') :\n",
        "  !gunzip 'GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "#\n",
        "# If you need to save space on your Google Drive, then you can uncomment the\n",
        "# code below and delete the now unneeded compressed files\n",
        "#\n",
        "if os.path.exists(os.path.join(working_directory, 'aclImdb_v1.tar')) :\n",
        "   print(\"Removing Stanford directory tarball to save space....\")\n",
        "   os.remove(os.path.join(working_directory, 'aclImdb_v1.tar'))\n",
        "   print(\"Removed\")\n",
        "\n",
        "# Check directory contents\n",
        "print(\"The files in my working directory are\", os.listdir(working_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x_Hg_SAHujF",
        "outputId": "926e5321-c98d-4e1e-9582-0db5e11b5cad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set current directory to working directory.... : /content/drive/MyDrive/Colab_Notebooks/Sem_2/Section_2\n",
            "The files in my working directory are ['GoogleNews-vectors-negative300.bin', 'aclImdb', 'prova.gdoc', 'prova cartella', 'model.png', 'cnn1_model.json', 'cnn1_weights.h5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General Assignment Tasks"
      ],
      "metadata": {
        "id": "aS4vTvBJOjfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    This is fairly generic code for sentiment analysis cleaning but\n",
        "    it comes down to splitting the data into positive and negative\n",
        "    sentiments and labelling it accordingly with target values for\n",
        "    sentiment, 1 for positive and 0 for negative\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    dataset = []\n",
        "\n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "\n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label,f.read()))\n",
        "\n",
        "    shuffle(dataset)\n",
        "\n",
        "    return(dataset)"
      ],
      "metadata": {
        "id": "OrOgvxy7OmV2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir(os.path.join(working_directory,\"aclImdb\"))\n",
        "# print(\"Acting on data in\",os.path.join(working_directory,'aclImdb') )\n",
        "dataset = pre_process_data(os.path.join(working_directory,\"aclImdb\",'train'))\n",
        "# Have a quick look at a sample\n",
        "dataset[42]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHLSS-SjOvNi",
        "outputId": "667afa31-8429-4e58-a643-e488146db101"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,\n",
              " 'I suppose that today this film has relevance because it was an early Sofia Loren film. She was 19 years old when the film was made in 1953.<br /><br />I viewed this film because I wanted to see some of Sofia Loren\\'s early work. I was surprised when she came on camera having had her skin bronzed over in brown makeup to resemble an Ethiopian princess. Surely, today, this would have been viewed as a slur and to be avoided in movie making. It actually became annoying watching Ms. Loren in skin color paint throughout the film.<br /><br />Yes, this film would have been better made if the real opera singers had made this movie. Then, the singing and the actual facial gestures of the real artists would have been apparent. I discount the comments by others about whether the real opera singers are older and heavier in weight.<br /><br />As beautiful as Ms. Loren was at age 19 and still is today, the film would have been better received as though it were being performed on the stage. After all, we don\\'t see beautiful young people on stage with \"old opera singers\" back stage singing from behind the curtain! Do not discount the success of using heavy-weight opera singers. One only has to refer to the most artistically produced television commercial for the J. G. Wentworth Company with the opera singers on stage singing so professionally the praises of the company\\'s product. This is one of the best and entertaining TV commercials produced to date.<br /><br />The quality of the movie print also makes this production of a somewhat lesser quality. The color ink has faded much and that can not be helped.<br /><br />To improve this film on DVD the production company should add English language subtitles so that we, who do not speak Italian, can know what the lyrics are saying. It would help the story and teach it more than the narrator giving 30 seconds of introduction to the scenes.<br /><br />Watch this film not because of the story of Aida nor the fact that this is an opera. Aside from Ms. Sofia Loren none of her co-actors are known nor remembered by this writer. Instead, watch this movie if you are a fan of Ms. Loren and wish to see her at age 19 -- no matter what the production is.<br /><br />Larry from Illinois')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit = 20000, binary=True)"
      ],
      "metadata": {
        "id": "Fl1TlPtqRjie"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(w[token])\n",
        "\n",
        "            except KeyError:\n",
        "                pass # No matching token in the downloaded Google word2vec vocab\n",
        "\n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data"
      ],
      "metadata": {
        "id": "5S_Dg_DDT0mu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\" Extract the target sentiments from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ],
      "metadata": {
        "id": "6z-Rd6eIT2uS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad or truncate each sequence of input tokens in each review so that we have\n",
        "# a fixed input length (**maxlen**) of tokens for network input\n",
        "\n",
        "def pad_trunc(data, maximumlen):\n",
        "    \"\"\"\n",
        "    Pad or truncate each review to the size set by the hyperparameter maxlen\n",
        "    because we need all the inputs of the CNN to be of fixed size.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maximumlen: # if the input is too large, truncate it\n",
        "            temp = sample[:maximumlen]\n",
        "        elif len(sample) < maximumlen: # if the input is too small, pad it\n",
        "            temp = sample\n",
        "            # Append the appropriate number zero vectors to the list\n",
        "            additional_elems = maximumlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "KB2FGEpeT5FB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "print(\"Number of vectorized training data -\", len(vectorized_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BsrAys0T7Ll",
        "outputId": "576de0c5-2ea4-4777-86d8-545d2935c95d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectorized training data - 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When we no longer need the dataset, we delete it\n",
        "# This is both good housekeeping and it stops\n",
        "# Colab from complaining about RAM or disk overrun\n",
        "del(dataset)"
      ],
      "metadata": {
        "id": "Zhtg6HMfUEId"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_point = int(len(vectorized_data)*.8)\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]\n",
        "\n",
        "print(\"Number of training vectors - \", len(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZvYbFk7UGtD",
        "outputId": "02e63df5-a133-441a-c5c0-8f9c397f91b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training vectors -  20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(vectorized_data) # Clear some more storage!\n",
        "print(\"Cleared out unneeded data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBz09rSmUITM",
        "outputId": "3ec89a2a-d6d9-47c4-a09a-1f4ea420a0a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared out unneeded data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 50\n",
        "batch_size = 16\n",
        "embedding_dims = 300 # This is fixed as Google News used 300 dimensional vectors\n",
        "filters = 125\n",
        "kernel_size = 1\n",
        "hidden_dims = 125\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "XFamRPkfUKlv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad or truncate the reviews and convert the inputs to an optimized\n",
        "# Numpy format\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)   # Does the same with the test cases\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "3AdKFrGvUMer"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let people know what you are doing\n",
        "print('Build CNN model for our MN5002 Advanced NLP course....')\n",
        "model_cnn1 = Sequential()\n",
        "model_cnn1.add(Conv1D(\n",
        "    filters,\n",
        "    kernel_size,\n",
        "    padding='valid',\n",
        "    activation='relu',\n",
        "    strides=1,\n",
        "    input_shape=(maxlen, embedding_dims))\n",
        ")\n",
        "model_cnn1.add(GlobalMaxPooling1D())\n",
        "model_cnn1.add(Dense(hidden_dims)) # This is provides a fully connected hidden layer\n",
        "model_cnn1.add(Dropout(0.2)) # A dropout of 0.2 is added here\n",
        "model_cnn1.add(Activation('relu')) # The Rectified Linear Unit activation function is used here\n",
        "model_cnn1.add(Dense(1)) # Add 1 dense layer\n",
        "model_cnn1.add(Activation('sigmoid')) # Apply a sigmoid to the output of that layer to take the output to between 0 and 1\n",
        "print('Model built')\n",
        "model_cnn1.summary()\n",
        "model_cnn1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Model compiled')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVasYSGsUPJb",
        "outputId": "859af868-bdfa-4749-cca5-e7d7d96fe7cd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build CNN model for our MN5002 Advanced NLP course....\n",
            "Model built\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 50, 125)           37625     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 125)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 125)               15750     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 125)               0         \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 125)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 126       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53501 (208.99 KB)\n",
            "Trainable params: 53501 (208.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model compiled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fu88chjUbn0",
        "outputId": "794fa13d-e9b4-4c6a-9dc5-191d95625c8a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5439 - accuracy: 0.7193 - val_loss: 0.4820 - val_accuracy: 0.7668\n",
            "Epoch 2/2\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4544 - accuracy: 0.7893 - val_loss: 0.4837 - val_accuracy: 0.7640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fcb7f29a650>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(working_directory)\n",
        "model_cnn1_structure = model_cnn1.to_json()\n",
        "with open(\"cnn1_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_cnn1_structure)\n",
        "model_cnn1.save_weights(\"cnn1_weights.h5\")"
      ],
      "metadata": {
        "id": "nsqVd_bIUdjZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the module to read in a JSON format model\n",
        "# Remove the lines below with set of 3 inverted commas to enable code\n",
        "\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "# Now instantiate a model\n",
        "\n",
        "os.chdir(working_directory)\n",
        "\n",
        "with open(\"cnn1_model.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model_cnn1 = model_from_json(json_string)\n",
        "\n",
        "# Once the model structure exists, set its characteristic weights\n",
        "\n",
        "model_cnn1.load_weights('cnn1_weights.h5')\n",
        "model_cnn1.summary()"
      ],
      "metadata": {
        "id": "Nv3uTyCtUgOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ea6896-478b-4fd6-9ebf-b96031b67ebb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 50, 125)           37625     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 125)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 125)               15750     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 125)               0         \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 125)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 126       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53501 (208.99 KB)\n",
            "Trainable params: 53501 (208.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Task 1"
      ],
      "metadata": {
        "id": "bESHQ0UsdMIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = 'this movie is terrible'\n",
        "print(test1, '- straightforward negative sentiment, used to check if the model works correctly\\n')\n",
        "test2 = 'this movie is wonderful, the best thing I\\'ve ever seen I believe it to be an undisputed masterpiece'\n",
        "print(test2, '- straightforward positive sentiment, used to check if the model works correctly\\n')\n",
        "test3 = 'this isn\\'t the best movie of all time'\n",
        "print(test3, '- negation of a positive sentiment, used to check the model\\'s reaction\\n')\n",
        "test4 = 'this movie is as good as The Room'\n",
        "print(test4, '- \\\"The Room\\\" is considered to be one of the worst movies ever made. This test is a trick\\n')\n",
        "test5 = 'how this movie is considered good will never fail to surprise me'\n",
        "print(test5, '- different type of trick: the reviewer is admitting that the movie is generally considered good, but they have an opposing opinion\\n')\n",
        "test6 = 'watching this movie will make you feel like holding a puppy or eating chocolate'\n",
        "print(test6, '- not explicit positive review\\n')\n",
        "test7 = 'Parasite is best described as a melancholy ghost story, albeit one disguised beneath umpteen layers of superbly designed (and impeccably photographed) generic mutations. Thrillingly played by a flawless ensemble cast who hit every note and harmonic resonance of Bong and co-writer Han Jin-won’s multitonal script, it’s a tragicomic masterclass that will get under your skin and eat away at your cinematic soul.'\n",
        "print(test7, '- long, glowing review (source: The Guardian)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YIl0AA7eKKX",
        "outputId": "9eae38c4-ed63-4d8b-daf6-9b1f9e4edc2e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this movie is terrible - straightforward negative sentiment, used to check if the model works correctly\n",
            "\n",
            "this movie is wonderful, the best thing I've ever seen I believe it to be an undisputed masterpiece - straightforward positive sentiment, used to check if the model works correctly\n",
            "\n",
            "this isn't the best movie of all time - negation of a positive sentiment, used to check the model's reaction\n",
            "\n",
            "this movie is as good as The Room - \"The Room\" is considered to be one of the worst movies ever made. This test is a trick\n",
            "\n",
            "how this movie is considered good will never fail to surprise me - different type of trick: the reviewer is admitting that the movie is generally considered good, but they have an opposing opinion\n",
            "\n",
            "watching this movie will make you feel like holding a puppy or eating chocolate - not explicit positive review\n",
            "\n",
            "Parasite is best described as a melancholy ghost story, albeit one disguised beneath umpteen layers of superbly designed (and impeccably photographed) generic mutations. Thrillingly played by a flawless ensemble cast who hit every note and harmonic resonance of Bong and co-writer Han Jin-won’s multitonal script, it’s a tragicomic masterclass that will get under your skin and eat away at your cinematic soul. - long, glowing review (source: The Guardian)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Task 2"
      ],
      "metadata": {
        "id": "e4r5T_XDlBoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test 5"
      ],
      "metadata": {
        "id": "tMGS3QphogcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a few attempts, it looks like the test that yelded the most accurate results was test 2, where all the hyperparameters were halved. In this attempt I halved all the hyperparameters, except for the epochs, to avoid undertraining the model."
      ],
      "metadata": {
        "id": "bGu3s5Jt3cNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a sample\n",
        "sample_2 =\"This is one of the worst movies I have ever seen. I am surprised anyone has recommended it, it is as bad as they get.\"\n",
        "\n",
        "vec_list = tokenize_and_vectorize([(1, sample_2)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TFkQPeGlDZp",
        "outputId": "e4c0955b-b5a3-4874-bca2-1b1d62d747ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15233505]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK\n",
        "\n",
        "This is an explicitly negative review, and the result is consistent with that."
      ],
      "metadata": {
        "id": "4Z57_012dD3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test1)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdu1KtHymLBW",
        "outputId": "5beb1b11-0648-4f52-8c97-3b07362f9aed"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13452852]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - OK\n",
        "\n",
        "Another explicitly negative review, correctly recognised."
      ],
      "metadata": {
        "id": "6afWXGKRdF1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test2)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7OBVg38mO_o",
        "outputId": "6d7b09e2-4a8d-4a24-85ca-272940e67ab5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9658818]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - OK\n",
        "\n",
        "This review is explicitly positive, and the result is very close to 1."
      ],
      "metadata": {
        "id": "uLeisH6cdKRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test3)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5guaAH3OnctI",
        "outputId": "1d7ba29a-4385-4a72-952d-92b03355d2cc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.88083464]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - NO\n",
        "\n",
        "This review is confusing: it is not meant to be strongly negative, but it is constructed in a way that could confuse the model. The review says that this is NOT the best movie of all time, but the model has recognised it as strongly positive."
      ],
      "metadata": {
        "id": "plDw_tcndR5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test4)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD3xE7I_ngH1",
        "outputId": "0d6c2e01-6403-43b1-e859-2353f4779bff"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.85251075]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative - NO\n",
        "\n",
        "This is another confusing review: the tone is ironic, with the reviewer saying it is as good as a movie that's universally considered to be terrible. The model fell into the trap and recognised it as a positive review, not knowing the pop-culture reference."
      ],
      "metadata": {
        "id": "1N0s2d3nddtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test5)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nBBeokQnh1o",
        "outputId": "b861959a-dc1a-4055-a307-cf30c532f56b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6730415]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split - NO\n",
        "\n",
        "In this case, the review states that the movie is considered good, but the reviewer disagrees with this sentiment. Doesn't understand the disagreement, and recognises it as positive, but not strongly."
      ],
      "metadata": {
        "id": "vWitjcOOdmc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test6)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmVkTI7njrO",
        "outputId": "f5bb3614-ae94-4f04-80f5-5a0daa83dd89"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55929637]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - NO\n",
        "\n",
        "This review is very positive, but not explicit. The reviewer compares watching the movie to a series of pleasurable activities, but it is never outwardly stated that the movie is good and there are no positive adjectives. The model recognises it as a neutral review."
      ],
      "metadata": {
        "id": "RnRiR0jQdo16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and vectorize it\n",
        "vec_list = tokenize_and_vectorize([(1, test7)]) # Feed a sample to the tokenizer and vectorizer\n",
        "\n",
        "# Convert the input to a fixed input length by padding or truncating\n",
        "test_vec_list = pad_trunc(vec_list, maxlen) # convert the input to a fixed input length\n",
        "\n",
        "# Input it to the model to predict the sentiment for it\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "# Present the prediction\n",
        "model_cnn1.predict(test_vec) # predict the sentiment of the input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPb6s5se2go",
        "outputId": "0c2949bd-53a3-4228-ec2a-13fcfbc6e092"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7029168]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive - OK\n",
        "\n",
        "This review is very positive. The model recognises it as such, but doesn't understand how stronly positive it is."
      ],
      "metadata": {
        "id": "8e3jEybD2N8_"
      }
    }
  ]
}